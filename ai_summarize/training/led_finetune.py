import os
import time
from datetime import datetime

import torch
import evaluate
import numpy as np
from datasets import Dataset

# --- Patch trÃ¡nh lá»—i dispatch_batches/even_batches vá»›i accelerate ---
import inspect
import accelerate
_sig = inspect.signature(accelerate.Accelerator.__init__)
if "dispatch_batches" not in _sig.parameters:
    _old_init = accelerate.Accelerator.__init__
    def _patched_init(self, *args, **kwargs):
        kwargs.pop("dispatch_batches", None)
        kwargs.pop("even_batches", None)
        return _old_init(self, *args, **kwargs)
    accelerate.Accelerator.__init__ = _patched_init
# -------------------------------------------------------------------

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
# Fallback an toÃ n náº¿u Seq2SeqTrainer khÃ´ng kháº£ dá»¥ng
from transformers import Trainer as BaseTrainer, TrainingArguments as BaseArgs
try:
    from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
    _HAS_S2S = True
except Exception:
    Seq2SeqTrainer = BaseTrainer
    Seq2SeqTrainingArguments = BaseArgs
    _HAS_S2S = False

from training.utils.train_history import log_eval
from training.utils.file_loader import load_all_data_from_folder


def train_led(data_folder: str):
    """Fine-tune BART/LED trÃªn thÆ° má»¥c chá»©a .txt/.docx/.pdf (dÃ¹ng ná»™i dung tá»± tÃ³m táº¯t)."""
    start_time = datetime.now().isoformat()
    t0 = time.time()

    print(f"ğŸš€ Fine-tuning LED/BART using data in folder: {data_folder}", flush=True)

    # 1) Model & tokenizer (báº¡n cÃ³ thá»ƒ Ä‘á»•i sang 'allenai/led-base-16384' náº¿u cáº§n input dÃ i)
    model_name = "facebook/bart-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    if tokenizer.pad_token is None:
        # BART Ä‘Ã£ cÃ³ pad_token, phÃ²ng há» trÆ°á»ng há»£p khÃ¡c
        tokenizer.pad_token = tokenizer.eos_token

    # 2) Load dá»¯ liá»‡u (Ä‘Ã£ chuáº©n hÃ³a thÃ nh [{'text': ..., 'summary': ...}, ...])
    data = load_all_data_from_folder(data_folder)
    if not data:
        raise ValueError("âŒ No valid training data found in folder.")
    raw_ds = Dataset.from_list(data)

    # 3) Tiá»n xá»­ lÃ½ cho batched=True
    def preprocess(batch):
        # batch["text"] & batch["summary"] lÃ  list[str]
        enc = tokenizer(
            batch["text"],
            max_length=512,             # Ä‘á»•i náº¿u muá»‘n input dÃ i hÆ¡n khi dÃ¹ng LED
            truncation=True,
            padding="max_length",
        )
        # dÃ¹ng text_target Ä‘á»ƒ mÃ£ hÃ³a label Ä‘Ãºng nhÃ¡nh decoder (khÃ´ng cáº§n as_target_tokenizer)
        lab = tokenizer(
            text_target=batch["summary"],
            max_length=128,
            truncation=True,
            padding="max_length",
        )
        enc["labels"] = lab["input_ids"]          # (batch, seq_len) list[list[int]]
        return enc

    ds = raw_ds.map(preprocess, batched=True, remove_columns=raw_ds.column_names)

    # 4) Metric (ROUGE)
    rouge = evaluate.load("rouge")

    def compute_metrics(eval_pred):
        """
        Robust vá»›i cáº£ 2 trÆ°á»ng há»£p:
        - Seq2SeqTrainer + predict_with_generate=True -> preds lÃ  token IDs [bsz, seq]
        - Fallback Trainer (hoáº·c cáº¥u hÃ¬nh khÃ¡c) -> cÃ³ thá»ƒ tráº£ vá» logits [bsz, seq, vocab]
        """
        preds, labels = eval_pred

        # Chuáº©n hoÃ¡ preds
        if isinstance(preds, tuple):
            preds = preds[0]
        preds = np.asarray(preds)
        if preds.ndim == 3:  # logits -> ids
            preds = preds.argmax(axis=-1)
        preds_list = preds.tolist()  # list[list[int]]

        # Chuáº©n hoÃ¡ labels: thay -100 báº±ng pad_token_id Ä‘á»ƒ decode
        labels = np.asarray(labels)
        pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
        labels = np.where(labels == -100, pad_id, labels)
        labels_list = labels.tolist()

        # Decode
        decoded_preds = tokenizer.batch_decode(preds_list, skip_special_tokens=True)
        decoded_labels = tokenizer.batch_decode(labels_list, skip_special_tokens=True)

        decoded_preds = [p.strip() for p in decoded_preds]
        decoded_labels = [l.strip() for l in decoded_labels]

        return rouge.compute(predictions=decoded_preds, references=decoded_labels)

    # 5) Training args
    # Vá»›i Seq2SeqTrainer, predict_with_generate=True Ä‘á»ƒ evaluate dÃ¹ng token-ids sinh tá»« generate()
    # Náº¿u fallback vá» Base Trainer, cá» nÃ y Ä‘Æ°á»£c bá» qua an toÃ n.
    args = Seq2SeqTrainingArguments(
        output_dir="outputs/tmp_led",
        per_device_train_batch_size=2,
        num_train_epochs=1,
        logging_steps=5,
        save_strategy="no",
        report_to="none",
        fp16=torch.cuda.is_available(),
        predict_with_generate=True,        # ğŸ”‘ Æ°u tiÃªn IDs thay vÃ¬ logits
        generation_max_length=128,         # Ä‘á»™ dÃ i khi generate cho eval
        generation_num_beams=1,            # nhanh & Ä‘Æ¡n giáº£n
        remove_unused_columns=True,
    )

    # 6) Trainer
    TrainerCls = Seq2SeqTrainer if _HAS_S2S else BaseTrainer
    trainer = TrainerCls(
        model=model,
        args=args,
        train_dataset=ds,
        eval_dataset=ds,                   # cÃ³ thá»ƒ Ä‘á»•i sang subset náº¿u muá»‘n nhanh hÆ¡n
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    # 7) Train & Eval
    print("ğŸ¯ Starting fine-tuning ...", flush=True)
    trainer.train()
    metrics = trainer.evaluate()

    runtime = round(time.time() - t0, 2)
    end_time = datetime.now().isoformat()

    metrics.update({
        "train_folder": data_folder,
        "epochs": 1,
        "runtime": runtime,
        "start_time": start_time,
        "end_time": end_time,
        "model_name": model_name,
    })

    # 8) Log + save
    try:
        log_eval("led", metrics)
        print("ğŸ“Š Evaluation log saved successfully.", flush=True)
    except Exception as e:
        print(f"âš ï¸ Failed to log evaluation: {e}", flush=True)

    os.makedirs("checkpoints/led_ft", exist_ok=True)
    model.save_pretrained("checkpoints/led_ft")
    tokenizer.save_pretrained("checkpoints/led_ft")

    print("ğŸ’¾ Saved fine-tuned LED model â†’ checkpoints/led_ft", flush=True)
    print(f"âœ… Completed in {runtime}s.", flush=True)
    return metrics
